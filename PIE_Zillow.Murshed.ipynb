{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "nabil.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "U8EtLHmVuL-F",
        "outputId": "0cb64574-653e-42e4-a8ba-bde15404b659"
      },
      "source": [
        "from lxml import html\n",
        "import requests\n",
        "import unicodecsv as csv\n",
        "import argparse\n",
        "import json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-16bcc88b3df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0municodecsv\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unicodecsv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKUTIIazuz4V",
        "outputId": "c1cccca2-1612-4a93-8d14-2ccbf64ab464"
      },
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTeDz7MJu7mE",
        "outputId": "424decd3-1143-4893-e16d-59ffaa09edef"
      },
      "source": [
        "!pip install selenium"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVmnsdysuL-U"
      },
      "source": [
        "def clean(text):\n",
        "    if text:\n",
        "        return ' '.join(' '.join(text).split())\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_headers():\n",
        "    # Creating headers.\n",
        "    headers = {'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "               'accept-encoding': 'gzip, deflate, sdch, br',\n",
        "               'accept-language': 'en-GB,en;q=0.8,en-US;q=0.6,ml;q=0.4',\n",
        "               'cache-control': 'max-age=0',\n",
        "               'upgrade-insecure-requests': '1',\n",
        "               'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'}\n",
        "    return headers\n",
        "\n",
        "\n",
        "def create_url(zipcode, filter):\n",
        "    # Creating Zillow URL based on the filter.\n",
        "\n",
        "    if filter == \"newest\":\n",
        "        url = \"https://www.zillow.com/homes/for_sale/{0}/0_singlestory/days_sort\".format(zipcode)\n",
        "    elif filter == \"cheapest\":\n",
        "        url = \"https://www.zillow.com/homes/for_sale/{0}/0_singlestory/pricea_sort/\".format(zipcode)\n",
        "    else:\n",
        "        url = \"https://www.zillow.com/homes/for_sale/{0}_rb/?fromHomePage=true&shouldFireSellPageImplicitClaimGA=false&fromHomePageTab=buy\".format(zipcode)\n",
        "    print(url)\n",
        "    return url\n",
        "\n",
        "\n",
        "def save_to_file(response):\n",
        "    # saving response to `response.html`\n",
        "\n",
        "    with open(\"response.html\", 'w', encoding=\"utf-8\") as fp:\n",
        "        fp.write(response.text)\n",
        "\n",
        "\n",
        "def write_data_to_csv(data):\n",
        "    # saving scraped data to csv.\n",
        "\n",
        "    with open(\"properties-%s.csv\" % (zipcode), 'wb') as csvfile:\n",
        "        fieldnames = ['title', 'address', 'city', 'state', 'postal_code', 'price', 'facts and features', 'real estate provider', 'url']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "\n",
        "def get_response(url):\n",
        "    # Getting response from zillow.com.\n",
        "\n",
        "    for i in range(5):\n",
        "        response = requests.get(url, headers=get_headers())\n",
        "        print(\"status code received:\", response.status_code)\n",
        "        if response.status_code != 200:\n",
        "            # saving response to file for debugging purpose.\n",
        "            save_to_file(response)\n",
        "            continue\n",
        "        else:\n",
        "            save_to_file(response)\n",
        "            return response\n",
        "    return None\n",
        "\n",
        "def get_data_from_json(raw_json_data):\n",
        "    # getting data from json (type 2 of their A/B testing page)\n",
        "\n",
        "    cleaned_data = clean(raw_json_data).replace('<!--', \"\").replace(\"-->\", \"\")\n",
        "    properties_list = []\n",
        "\n",
        "    try:\n",
        "        json_data = json.loads(cleaned_data)\n",
        "        search_results = json_data.get('searchResults').get('listResults', [])\n",
        "\n",
        "        for properties in search_results:\n",
        "            address = properties.get('addressWithZip')\n",
        "            property_info = properties.get('hdpData', {}).get('homeInfo')\n",
        "            city = property_info.get('city')\n",
        "            state = property_info.get('state')\n",
        "            postal_code = property_info.get('zipcode')\n",
        "            price = properties.get('price')\n",
        "            bedrooms = properties.get('beds')\n",
        "            bathrooms = properties.get('baths')\n",
        "            area = properties.get('area')\n",
        "            info = f'{bedrooms} bds, {bathrooms} ba ,{area} sqft'\n",
        "            broker = properties.get('brokerName')\n",
        "            property_url = properties.get('detailUrl')\n",
        "            title = properties.get('statusText')\n",
        "\n",
        "            data = {'address': address,\n",
        "                    'city': city,\n",
        "                    'state': state,\n",
        "                    'postal_code': postal_code,\n",
        "                    'price': price,\n",
        "                    'facts and features': info,\n",
        "                    'real estate provider': broker,\n",
        "                    'url': property_url,\n",
        "                    'title': title}\n",
        "            properties_list.append(data)\n",
        "\n",
        "        return properties_list\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid json\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse(zipcode, filter=None):\n",
        "    url = create_url(zipcode, filter)\n",
        "    response = get_response(url)\n",
        "\n",
        "    if not response:\n",
        "        print(\"Failed to fetch the page, please check `response.html` to see the response received from zillow.com.\")\n",
        "        return None\n",
        "\n",
        "    parser = html.fromstring(response.text)\n",
        "    search_results = parser.xpath(\"//div[@id='search-results']//article\")\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"parsing from json data\")\n",
        "        # identified as type 2 page\n",
        "        raw_json_data = parser.xpath('//script[@data-zrr-shared-data-key=\"mobileSearchPageStore\"]//text()')\n",
        "        return get_data_from_json(raw_json_data)\n",
        "\n",
        "    print(\"parsing from html page\")\n",
        "    properties_list = []\n",
        "    for properties in search_results:\n",
        "        raw_address = properties.xpath(\".//span[@itemprop='address']//span[@itemprop='streetAddress']//text()\")\n",
        "        raw_city = properties.xpath(\".//span[@itemprop='address']//span[@itemprop='addressLocality']//text()\")\n",
        "        raw_state = properties.xpath(\".//span[@itemprop='address']//span[@itemprop='addressRegion']//text()\")\n",
        "        raw_postal_code = properties.xpath(\".//span[@itemprop='address']//span[@itemprop='postalCode']//text()\")\n",
        "        raw_price = properties.xpath(\".//span[@class='zsg-photo-card-price']//text()\")\n",
        "        raw_info = properties.xpath(\".//span[@class='zsg-photo-card-info']//text()\")\n",
        "        raw_broker_name = properties.xpath(\".//span[@class='zsg-photo-card-broker-name']//text()\")\n",
        "        url = properties.xpath(\".//a[contains(@class,'overlay-link')]/@href\")\n",
        "        raw_title = properties.xpath(\".//h4//text()\")\n",
        "\n",
        "        address = clean(raw_address)\n",
        "        city = clean(raw_city)\n",
        "        state = clean(raw_state)\n",
        "        postal_code = clean(raw_postal_code)\n",
        "        price = clean(raw_price)\n",
        "        info = clean(raw_info).replace(u\"\\xb7\", ',')\n",
        "        broker = clean(raw_broker_name)\n",
        "        title = clean(raw_title)\n",
        "        property_url = \"https://www.zillow.com\" + url[0] if url else None\n",
        "        is_forsale = properties.xpath('.//span[@class=\"zsg-icon-for-sale\"]')\n",
        "\n",
        "        properties = {'address': address,\n",
        "                      'city': city,\n",
        "                      'state': state,\n",
        "                      'postal_code': postal_code,\n",
        "                      'price': price,\n",
        "                      'facts and features': info,\n",
        "                      'real estate provider': broker,\n",
        "                      'url': property_url,\n",
        "                      'title': title}\n",
        "        if is_forsale:\n",
        "            properties_list.append(properties)\n",
        "    return properties_list"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzuFdch8uL-X",
        "outputId": "cd0598bd-5da8-4c44-b9ec-a1cfecf67682"
      },
      "source": [
        "zipcode = '10040'\n",
        "sort = 'newest'\n",
        "print (\"Fetching data for %s\" % (zipcode))\n",
        "scraped_data = parse(zipcode, sort)\n",
        "if scraped_data:\n",
        "    print(\"Writing data to output file\")\n",
        "    write_data_to_csv(scraped_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching data for 10040\n",
            "https://www.zillow.com/homes/for_sale/10040/0_singlestory/days_sort\n",
            "status code received: 200\n",
            "parsing from json data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'get'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-18-32b791b90e14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'newest'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Fetching data for %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mzipcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscraped_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscraped_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Writing data to output file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-17-cf145a333208>\u001b[0m in \u001b[0;36mparse\u001b[1;34m(zipcode, filter)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# identified as type 2 page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mraw_json_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//script[@data-zrr-shared-data-key=\"mobileSearchPageStore\"]//text()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_data_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_json_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"parsing from html page\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-17-cf145a333208>\u001b[0m in \u001b[0;36mget_data_from_json\u001b[1;34m(raw_json_data)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mjson_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'searchResults'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'listResults'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mproperties\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1VikQ45uL-Z"
      },
      "source": [
        "def parse(zipcode,filter=None):\n",
        "    if filter==\"newest\":\n",
        "        url = \"https://www.zillow.com/homes/for_sale/{0}/0_singlestory/days_sort\".format(zipcode)\n",
        "    elif filter == \"cheapest\":\n",
        "        url = \"https://www.zillow.com/homes/for_sale/{0}/0_singlestory/pricea_sort/\".format(zipcode)\n",
        "    else:\n",
        "        url = \"https://www.zillow.com/homes/for_sale/{0}_rb/?fromHomePage=true&shouldFireSellPageImplicitClaimGA=false&fromHomePageTab=buy\".format(zipcode)\n",
        "\n",
        "    for i in range(5):\n",
        "        # try:\n",
        "        headers= {\n",
        "                'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "                'accept-encoding':'gzip, deflate, sdch, br',\n",
        "                'accept-language':'en-GB,en;q=0.8,en-US;q=0.6,ml;q=0.4',\n",
        "                'cache-control':'max-age=0',\n",
        "                'upgrade-insecure-requests':'1',\n",
        "                'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
        "    }\n",
        "    response = requests.get(url,headers=headers)\n",
        "    print(response.status_code)\n",
        "    parser = html.fromstring(response.text)\n",
        "    search_results = parser.xpath(\"//div[@id='search-results']//article\")\n",
        "    properties_list = []\n",
        "\t\t\n",
        "    for properties in search_results:\n",
        "        raw_address = properties.xpath(\".//span[@itemprop='address']//span[@itemprop='streetAddress']//text()\")\n",
        "        raw_city = properties.xpath(\".//span[@itemprop='address']//span[@itemprop='addressLocality']//text()\")\n",
        "        raw_state= properties.xpath(\".//span[@itemprop='address']//span[@itemprop='addressRegion']//text()\")\n",
        "        raw_postal_code= properties.xpath(\".//span[@itemprop='address']//span[@itemprop='postalCode']//text()\")\n",
        "        raw_price = properties.xpath(\".//span[@class='zsg-photo-card-price']//text()\")\n",
        "        raw_info = properties.xpath(\".//span[@class='zsg-photo-card-info']//text()\")\n",
        "        raw_broker_name = properties.xpath(\".//span[@class='zsg-photo-card-broker-name']//text()\")\n",
        "        url = properties.xpath(\".//a[contains(@class,'overlay-link')]/@href\")\n",
        "        raw_title = properties.xpath(\".//h4//text()\")\n",
        "\n",
        "        address = ' '.join(' '.join(raw_address).split()) if raw_address else None\n",
        "        city = ''.join(raw_city).strip() if raw_city else None\n",
        "        state = ''.join(raw_state).strip() if raw_state else None\n",
        "        postal_code = ''.join(raw_postal_code).strip() if raw_postal_code else None\n",
        "        price = ''.join(raw_price).strip() if raw_price else None\n",
        "        info = ' '.join(' '.join(raw_info).split()).replace(u\"\\xb7\",',')\n",
        "        broker = ''.join(raw_broker_name).strip() if raw_broker_name else None\n",
        "        title = ''.join(raw_title) if raw_title else None\n",
        "        property_url = \"https://www.zillow.com\"+url[0] if url else None \n",
        "        is_forsale = properties.xpath('.//span[@class=\"zsg-icon-for-sale\"]')\n",
        "        properties = {\n",
        "                        'address':address,\n",
        "                        'city':city,\n",
        "                        'state':state,\n",
        "                        'postal_code':postal_code,\n",
        "                        'price':price,\n",
        "                        'facts and features':info,\n",
        "                        'real estate provider':broker,\n",
        "                        'url':property_url,\n",
        "                        'title':title\n",
        "        }\n",
        "        if is_forsale:\n",
        "            properties_list.append(properties)\n",
        "    return properties_list\n",
        "    # except:\n",
        "    # \tprint (\"Failed to process the page\",url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyN7Egd2uL-b",
        "outputId": "86593f2d-a386-4822-a567-db9f394f81bf"
      },
      "source": [
        "zipcode = '10040'\n",
        "sort = 'Homes For You'\n",
        "print (\"Fetching data for %s\"%(zipcode))\n",
        "scraped_data = parse(zipcode,sort)\n",
        "print (\"Writing data to output file\")\n",
        "with open(\"properties-%s.csv\"%(zipcode),'wb')as csvfile:\n",
        "    fieldnames = ['title','address','city','state','postal_code','price','facts and features','real estate provider','url']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for row in  scraped_data:\n",
        "        writer.writerow(row)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching data for 10040\n",
            "200\n",
            "Writing data to output file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb1Go2MauL-f",
        "outputId": "c7403af7-f28c-4d46-bbaa-4fe2a2dcf6b2"
      },
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import requests\n",
        "import lxml\n",
        "from lxml.html.soupparser import fromstring\n",
        "import prettify\n",
        "import numbers\n",
        "\n",
        "#set some display settings for notebooks\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "#add headers in case you use chromedriver (captchas are no fun); namely used for chromedriver\n",
        "req_headers = {\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
        "    'accept-encoding': 'gzip, deflate, br',\n",
        "    'accept-language': 'en-US,en;q=0.8',\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
        "}\n",
        "\n",
        "#create url variables for each zillow page, at a time 10 page oepner\n",
        "# accessing 10 page, sending request to 10 pages\n",
        "with requests.Session() as s:\n",
        "    city = 'atlanta/'  ## can be zip, or address, or street name\n",
        "    \n",
        "    url = 'https://www.zillow.com/homes/for_sale/'+city\n",
        "    url2 = 'https://www.zillow.com/homes/for_sale/'+city+'/2_p/'\n",
        "    url3 = 'https://www.zillow.com/homes/for_sale/'+city+'/3_p/'\n",
        "    url4 = 'https://www.zillow.com/homes/for_sale/'+city+'/4_p/'\n",
        "    url5 = 'https://www.zillow.com/homes/for_sale/'+city+'/5_p/'\n",
        "    url6 = 'https://www.zillow.com/homes/for_sale/'+city+'/6_p/'\n",
        "    url7 = 'https://www.zillow.com/homes/for_sale/'+city+'/7_p/'\n",
        "    url8 = 'https://www.zillow.com/homes/for_sale/'+city+'/8_p/'\n",
        "    url9 = 'https://www.zillow.com/homes/for_sale/'+city+'/9_p/'\n",
        "    url10 = 'https://www.zillow.com/homes/for_sale/'+city+'/10_p/'\n",
        "\n",
        "    r = s.get(url, headers=req_headers)\n",
        "    r2 = s.get(url2, headers=req_headers)\n",
        "    r3 = s.get(url3, headers=req_headers)\n",
        "    r4 = s.get(url4, headers=req_headers)\n",
        "    r5 = s.get(url5, headers=req_headers)\n",
        "    r6 = s.get(url6, headers=req_headers)\n",
        "    r7 = s.get(url7, headers=req_headers)\n",
        "    r8 = s.get(url8, headers=req_headers)\n",
        "    r9 = s.get(url9, headers=req_headers)\n",
        "    r10 = s.get(url10, headers=req_headers)\n",
        "    \n",
        "    url_links = [url, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
        "\n",
        "#add contents of urls to soup variable from each url\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "soup1 = BeautifulSoup(r2.content, 'html.parser')\n",
        "soup2 = BeautifulSoup(r3.content, 'html.parser')\n",
        "soup3 = BeautifulSoup(r4.content, 'html.parser')\n",
        "soup4 = BeautifulSoup(r5.content, 'html.parser')\n",
        "soup5 = BeautifulSoup(r6.content, 'html.parser')\n",
        "soup6 = BeautifulSoup(r7.content, 'html.parser')\n",
        "soup7 = BeautifulSoup(r8.content, 'html.parser')\n",
        "soup8 = BeautifulSoup(r9.content, 'html.parser')\n",
        "soup9 = BeautifulSoup(r10.content, 'html.parser')\n",
        "\n",
        "# page_links = [soup, soup1, soup2, soup3, soup4, soup5, soup6, soup7, soup8, soup9]\n",
        "\n",
        "#create the first two dataframes\n",
        "df = pd.DataFrame()\n",
        "df1 = pd.DataFrame()\n",
        "\n",
        "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
        "for i in soup:\n",
        "    address = soup.find_all (class_= 'list-card-addr')\n",
        "    price = list(soup.find_all (class_='list-card-price'))\n",
        "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link = soup.find_all (class_= 'list-card-link')\n",
        "    \n",
        "    #create dataframe columns out of variables\n",
        "    df['prices'] = price\n",
        "    df['address'] = address\n",
        "    df['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag a\n",
        "for link in soup.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df['links'] = urls\n",
        "df['links'] = df['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "\n",
        "\n",
        "for i in soup1:\n",
        "    address1 = soup1.find_all (class_= 'list-card-addr')\n",
        "    price1 = list(soup1.find_all (class_='list-card-price'))\n",
        "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details1 = soup1.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type1 = soup1.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated1 = soup1.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage1 = list(soup1.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link1 = soup1.find_all (class_= 'list-card-link')\n",
        "\n",
        "    #create dataframe columns out of variables\n",
        "    df1['prices'] = price1\n",
        "    df1['address'] = address1\n",
        "    df1['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup1.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df1['links'] = urls\n",
        "df1['links'] = df1['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df1['links'] = df1['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df1['links'] = df1['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "\n",
        "#append first two dataframes\n",
        "df = df.append(df1, ignore_index = True) \n",
        "\n",
        "#create empty dataframes\n",
        "df2 = pd.DataFrame()\n",
        "df3 = pd.DataFrame()\n",
        "df4 = pd.DataFrame()\n",
        "df5 = pd.DataFrame()\n",
        "df6 = pd.DataFrame()\n",
        "df7 = pd.DataFrame()\n",
        "df8 = pd.DataFrame()\n",
        "df9 = pd.DataFrame()\n",
        "\n",
        "for i in soup2:\n",
        "    soup = soup2\n",
        "    address = soup.find_all (class_= 'list-card-addr')\n",
        "    price = list(soup.find_all (class_='list-card-price'))\n",
        "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link = soup.find_all (class_= 'list-card-link')\n",
        "    \n",
        "    #create dataframe columns out of variables\n",
        "    df2['prices'] = price\n",
        "    df2['address'] = address\n",
        "    df2['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup2.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df2['links'] = urls\n",
        "df2['links'] = df2['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df2['links'] = df2['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df2['links'] = df2['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "\n",
        "    \n",
        "for i in soup3:\n",
        "    soup = soup3\n",
        "    address1 = soup.find_all (class_= 'list-card-addr')\n",
        "    price1 = list(soup.find_all (class_='list-card-price'))\n",
        "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link1 = soup.find_all (class_= 'list-card-link')\n",
        "\n",
        "    #create dataframe columns out of variables\n",
        "    df3['prices'] = price1\n",
        "    df3['address'] = address1\n",
        "    df3['beds'] = beds1\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup3.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df3['links'] = urls\n",
        "df3['links'] = df3['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df3['links'] = df3['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df3['links'] = df3['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "\n",
        "for i in soup4:\n",
        "    soup = soup4\n",
        "    address1 = soup.find_all (class_= 'list-card-addr')\n",
        "    price1 = list(soup.find_all (class_='list-card-price'))\n",
        "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link1 = soup.find_all (class_= 'list-card-link')\n",
        "\n",
        "    #create dataframe columns out of variables\n",
        "    df4['prices'] = price1\n",
        "    df4['address'] = address1\n",
        "    df4['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup4.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df4['links'] = urls\n",
        "df4['links'] = df4['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df4['links'] = df4['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df4['links'] = df4['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "\n",
        "for i in soup5:\n",
        "    soup = soup5\n",
        "    address1 = soup.find_all (class_= 'list-card-addr')\n",
        "    price1 = list(soup.find_all (class_='list-card-price'))\n",
        "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link1 = soup.find_all (class_= 'list-card-link')\n",
        "\n",
        "    #create dataframe columns out of variables\n",
        "    df5['prices'] = price1\n",
        "    df5['address'] = address1\n",
        "    df5['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup5.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df5['links'] = urls\n",
        "df5['links'] = df5['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df5['links'] = df5['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df5['links'] = df5['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "    \n",
        "for i in soup6:\n",
        "    soup = soup6\n",
        "    address1 = soup.find_all (class_= 'list-card-addr')\n",
        "    price1 = list(soup.find_all (class_='list-card-price'))\n",
        "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link1 = soup.find_all (class_= 'list-card-link')\n",
        "\n",
        "    #create dataframe columns out of variables\n",
        "    df6['prices'] = price1\n",
        "    df6['address'] = address1\n",
        "    df6['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup6.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df6['links'] = urls\n",
        "df6['links'] = df6['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df6['links'] = df6['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df6['links'] = df6['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "    \n",
        "for i in soup7:\n",
        "    soup = soup7\n",
        "    address1 = soup.find_all (class_= 'list-card-addr')\n",
        "    price1 = list(soup.find_all (class_='list-card-price'))\n",
        "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link1 = soup.find_all (class_= 'list-card-link')\n",
        "\n",
        "    #create dataframe columns out of variables\n",
        "    df7['prices'] = price1\n",
        "    df7['address'] = address1\n",
        "    df7['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup7.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df7['links'] = urls\n",
        "df7['links'] = df7['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df7['links'] = df7['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df7['links'] = df7['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "    \n",
        "for i in soup8:\n",
        "    soup = soup8\n",
        "    address1 = soup.find_all (class_= 'list-card-addr')\n",
        "    price1 = list(soup.find_all (class_='list-card-price'))\n",
        "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link1 = soup.find_all (class_= 'list-card-link')\n",
        "\n",
        "    #create dataframe columns out of variables\n",
        "    df8['prices'] = price1\n",
        "    df8['address'] = address1\n",
        "    df8['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup8.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df8['links'] = urls\n",
        "df8['links'] = df8['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df8['links'] = df8['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df8['links'] = df8['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "\n",
        "for i in soup9:\n",
        "    soup = soup9\n",
        "    address1 = soup.find_all (class_= 'list-card-addr')\n",
        "    price1 = list(soup.find_all (class_='list-card-price'))\n",
        "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
        "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
        "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
        "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
        "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
        "    link1 = soup.find_all (class_= 'list-card-link')\n",
        "\n",
        "    #create dataframe columns out of variables\n",
        "    df9['prices'] = price1\n",
        "    df9['address'] = address1\n",
        "    df9['beds'] = beds\n",
        "\n",
        "#create empty url list\n",
        "urls = []\n",
        "\n",
        "#loop through url, pull the href and strip out the address tag\n",
        "for link in soup9.find_all(\"article\"):\n",
        "    href = link.find('a',class_=\"list-card-link\")\n",
        "    addresses = href.find('address')\n",
        "    addresses.extract()\n",
        "    urls.append(href)\n",
        "\n",
        "#import urls into a links column\n",
        "df9['links'] = urls\n",
        "df9['links'] = df9['links'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df9['links'] = df9['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
        "df9['links'] = df9['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
        "\n",
        "df = df.append(df2, ignore_index = True) \n",
        "df = df.append(df3, ignore_index = True) \n",
        "df = df.append(df4, ignore_index = True) \n",
        "df = df.append(df5, ignore_index = True) \n",
        "df = df.append(df6, ignore_index = True) \n",
        "df = df.append(df7, ignore_index = True) \n",
        "df = df.append(df8, ignore_index = True) \n",
        "df = df.append(df9, ignore_index = True) \n",
        "\n",
        "#convert columns to str\n",
        "df['prices'] = df['prices'].astype('str')\n",
        "df['address'] = df['address'].astype('str')\n",
        "df['beds'] = df['beds'].astype('str')\n",
        "\n",
        "#remove html tags\n",
        "df['prices'] = df['prices'].replace('<div class=\"list-card-price\">', ' ', regex=True)\n",
        "df['address'] = df['address'].replace('<address class=\"list-card-addr\">', ' ', regex=True)\n",
        "df['prices'] = df['prices'].replace('</div>', ' ', regex=True)\n",
        "df['address'] = df['address'].replace('</address>', ' ', regex=True)\n",
        "df['prices'] = df['prices'].str.replace(r'\\D', '')\n",
        "\n",
        "#remove html tags from beds column\n",
        "df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li>', ' ', regex=True)\n",
        "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li>', ' ', regex=True)\n",
        "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li>', ' ', regex=True)\n",
        "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bd</abbr></li><li>', ' ', regex=True)\n",
        "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li></ul>', ' ', regex=True)\n",
        "df['beds'] = df['beds'].replace('Studio</li><li>', '0 ', regex=True)\n",
        "\n",
        "#split beds column into beds, bath and sq_feet\n",
        "#df[['beds']] = df.beds.str.split(expand=True)\n",
        "\n",
        "#remove commas from sq_feet and convert to float\n",
        "df.replace(',','', regex=True, inplace=True)\n",
        "\n",
        "#drop nulls\n",
        "df = df[(df['prices'] != '') & (df['prices']!= ' ')]\n",
        "\n",
        "#convert column to float\n",
        "df['prices'] = df['prices'].astype('float')\n",
        "#df['sq_feet'] = df['sq_feet'].astype('float')\n",
        "\n",
        "print('The column datatypes are:')\n",
        "print(df.dtypes)\n",
        "print('The dataframe shape is:', df.shape)\n",
        "\n",
        "#rearrange the columns\n",
        "#df = df[['prices', 'address', 'links', 'beds', 'baths', 'sq_feet']]\n",
        "\n",
        "df.to_csv(\"PIE.zillow.csv\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The column datatypes are:\n",
            "prices     float64\n",
            "address     object\n",
            "beds        object\n",
            "links       object\n",
            "dtype: object\n",
            "The dataframe shape is: (400, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkVJU-FIhhSo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHJEtjXzvLJa",
        "outputId": "85f8e8d8-1c5f-45fe-90ff-7432ab4de8cb"
      },
      "source": [
        "!pip install prettify"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting prettify\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/41/4b8104d757fa78949eb38eaa74a6e4046fcb8d67ea85e762361c175423a8/prettify-0.1.1.tar.gz\n",
            "Building wheels for collected packages: prettify\n",
            "  Building wheel for prettify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prettify: filename=prettify-0.1.1-cp36-none-any.whl size=8424 sha256=020932a88684bc10a4ea8669843ad1bcc86018806e1250d45978441c6aa47c36\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/44/c3/f997cb138383a4a0399a6b2f75012746c533a24ff6b02fafe0\n",
            "Successfully built prettify\n",
            "Installing collected packages: prettify\n",
            "Successfully installed prettify-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDngsaW_uL-j",
        "outputId": "b1b1ef32-8210-4939-cd12-5406acd8aaca"
      },
      "source": [
        "!pip install selenium"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}